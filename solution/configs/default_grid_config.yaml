# Default configuration file for grid-based PLDM models
# This configuration uses the grid-based state representation

# General settings
seed: 101  # Random seed for reproducibility

# Workflow control
workflow:
  run_training: true   # Enable training phase
  run_probing: true    # Enable probing phase  
  run_planning: true   # Enable planning phase

# Data parameters
data:
  train_data_path: "./data/2020_hh_trials.csv"
  val_data_path: null  # Will use split from train data
  test_data_path: null  # Will use split from train data
  val_ratio: 0.1
  test_ratio: 0.1
  grid_size: 5

# Model parameters
model:
  type: "grid"
  encoder_type: "cnn"
  # grid encoder params
  grid_dims:
    H: 5
    W: 5
  in_channels: 26
  num_objects: 17
  hidden_channels: [16, 32, 64]
  fc_dims: [256, 128]
  
  # Predictor settings
  dynamics_predictor:
    predictor_type: "grid"  # Options: "grid", "transformer", "lstm"
    hidden_size: 128
    num_layers: 3
    dropout: 0.0
    activation: "relu"
    # Transformer-specific parameters
    nhead: 4  # Number of attention heads (transformer only)
    num_encoder_layers: 2  # Number of transformer encoder layers
    dim_feedforward: 512  # Feedforward dimension in transformer
    # LSTM-specific parameters
    bidirectional: false  # Whether to use bidirectional LSTM
  
  reward_predictor:
    predictor_type: "grid"  # Options: "grid", "transformer", "lstm"
    hidden_size: 64
    num_layers: 3
    dropout: 0.0
    activation: "relu"
    # Transformer-specific parameters
    nhead: 4  # Number of attention heads (transformer only)
    num_encoder_layers: 2  # Number of transformer encoder layers
    dim_feedforward: 512  # Feedforward dimension in transformer
    # LSTM-specific parameters
    bidirectional: false  # Whether to use bidirectional LSTM
  
  # Teacher forcing parameters
  teacher_forcing_ratio: 0.5  # Probability of using teacher forcing during training (0.0 to 1.0) #only for transformer predictor
  
  # reward predictor params
  reward_hidden_dims: [128, 64, 32]
  reward_activation: "relu"
  # dynamics predictor params
  dynamics_hidden_dims: [128, 64, 32]
  dynamics_activation: "relu"
  # general params
  activation: "relu"
  use_layer_norm: false
  device: "cuda"

# Loss function parameters
loss:
  dynamics_loss: "mse"  # Options: "mse" or "vicreg"
  reward_loss: "mse"    # Options: "mse" or "vicreg"
  
  # VICReg specific parameters (used when dynamics_loss or reward_loss is "vicreg")
  vicreg:
    projector_type: "identity"  # Options: "mlp" or "identity"
    projector_layers: [2048, 2048, 2048]  # Projector MLP hidden dimensions (used only if projector_type is "mlp")
    output_dim: 256                      # Projector output dimension (used only if projector_type is "mlp")
    sim_coeff: 25.0                      # Coefficient for similarity loss
    std_coeff: 25.0                      # Coefficient for std loss
    cov_coeff: 1.0                       # Coefficient for covariance loss
    std_margin: 1.0                      # Margin for std loss
    # Temporal coefficients (if using temporal data)
    sim_coeff_t: 10.0                    # Coefficient for temporal similarity loss
    std_coeff_t: 5.0                     # Coefficient for temporal std loss
    cov_coeff_t: 0.5                     # Coefficient for temporal covariance loss
    std_margin_t: 1.0                    # Margin for temporal std loss
    adjust_cov: true                     # Whether to adjust covariance loss

# Training parameters
training:
  batch_size: 128
  dynamics_epochs: 10
  reward_epochs: 5
  learning_rate: 0.001
  optimizer: "adam"
  weight_decay: 0.00001
  scheduler: "plateau"
  scheduler_patience: 10
  scheduler_factor: 0.5
  scheduler_min_lr: 0.000001
  max_norm: 1.0
  use_layer_norm: false
  seed: 42
  log_interval: 10
  num_workers: 4

# Testing parameters
testing:
  batch_size: 128
  use_val_data: false
  use_test_data: true
  num_samples: 100
  planning_horizon: 5
  planning_samples: 100

# WandB parameters
wandb:
  project: "pldm-overcooked"
  name: base
  entity: harshsutariya1179
  use_wandb: true  # Disabling WandB due to SSL certificate issues
  watch_model: true
  save_model: true
  
probing:
  # Probing targets
  targets: ["agent_pos"]
  probe_dynamics: true
  probe_reward: false
  
  # Model parameters
  hidden_dims: [128, 64]
  activation: "relu"
  
  # Training parameters
  batch_size: 64
  epochs: 20
  lr: 0.001
  val_ratio: 0.1
  test_ratio: 0.1
  
  # Visualization settings
  visualize: true
  max_vis_samples: 10