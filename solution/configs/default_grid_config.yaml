# Default configuration file for grid-based PLDM models
# This configuration uses the grid-based state representation

# General settings
seed: 101  # Random seed for reproducibility

# Workflow control
workflow:
  run_training: true   # Enable training phase
  run_probing: true    # Enable probing phase  
  run_planning: false   # Enable planning phase

# Data parameters
data:
  train_data_path: "/scratch/hs5580/ddrl/overcooked/data/2020_hh_trials.csv"
  val_data_path: null  # Will use split from train data
  test_data_path: null  # Will use split from train data
  val_ratio: 0.1
  test_ratio: 0.1
  grid_size: 5

# Model parameters
model:
  type: "grid"
  encoder_type: "cnn"  # Options: "cnn", "basic", etc.
  # CNN encoder params
  state_embed_dim: 128
  # grid encoder params
  grid_dims:
    H: 5
    W: 5
  in_channels: 32
  num_objects: 17
  hidden_channels: [16, 32, 64]
  fc_dims: [256, 128]
  
  # Predictor settings
  dynamics_predictor:
    predictor_type: "transformer"  # Options: "grid", "transformer", "lstm"
    hidden_size: 128
    num_layers: 3
    dropout: 0.15
    activation: "relu"
    # Transformer-specific parameters
    nhead: 4  # Number of attention heads (transformer only)
    num_encoder_layers: 2  # Number of transformer encoder layers
    dim_feedforward: 512  # Feedforward dimension in transformer
    # LSTM-specific parameters
    bidirectional: false  # Whether to use bidirectional LSTM
  
  reward_predictor:
    predictor_type: "grid"  # Options: "grid", "transformer", "lstm"
    hidden_size: 64
    num_layers: 3
    dropout: 0.15
    activation: "relu"
    # Transformer-specific parameters
    nhead: 4  # Number of attention heads (transformer only)
    num_encoder_layers: 2  # Number of transformer encoder layers
    dim_feedforward: 512  # Feedforward dimension in transformer
    # LSTM-specific parameters
    bidirectional: false  # Whether to use bidirectional LSTM
  
  # Teacher forcing parameters
  teacher_forcing_ratio: 0.5  # Probability of using teacher forcing during training (0.0 to 1.0) #only for transformer predictor
  
  # reward predictor params
  reward_hidden_dims: [128, 64, 32]
  reward_activation: "relu"
  # dynamics predictor params
  dynamics_hidden_dims: [128, 64, 32]
  dynamics_activation: "relu"
  # general params
  activation: "relu"
  use_layer_norm: false
  device: "cuda"

# Loss function parameters
loss:
  dynamics_loss: "vicreg"  # Options: "mse" or "vicreg"
  reward_loss: "mse"    # Options: "mse" or "vicreg"
  
  # VICReg specific parameters (used when dynamics_loss or reward_loss is "vicreg")
  vicreg:
    projector_type: "identity"  # Options: "mlp" or "identity"
    projector_layers: [2048, 2048, 2048]  # Projector MLP hidden dimensions (used only if projector_type is "mlp")
    output_dim: 256                      # Projector output dimension (used only if projector_type is "mlp")
    sim_coeff: 1.0                      # Coefficient for similarity loss
    std_coeff: 4.0                      # Coefficient for std loss
    cov_coeff: 7                       # Coefficient for covariance loss
    std_margin: 1.0                      # Margin for std loss
    # Temporal coefficients (if using temporal data)
    sim_coeff_t: 1.0                    # Coefficient for temporal similarity loss
    std_coeff_t: 5.0                     # Coefficient for temporal std loss
    cov_coeff_t: 0                     # Coefficient for temporal covariance loss
    std_margin_t: 1.0                    # Margin for temporal std loss
    adjust_cov: true                     # Whether to adjust covariance loss

# Training parameters
training:
  batch_size: 128
  dynamics_epochs: 1
  reward_epochs: 1
  learning_rate: 0.001
  optimizer: "adam"
  weight_decay: 0.00001
  scheduler: "plateau"
  scheduler_patience: 10
  scheduler_factor: 0.5
  scheduler_min_lr: 0.000001
  max_norm: 1.0
  use_layer_norm: false
  seed: 42
  log_interval: 10
  num_workers: 4
  # Model selection for training
  train_dynamics: true    # Whether to train the dynamics model
  train_reward: false      # Whether to train the reward model

# Testing parameters
testing:
  batch_size: 128
  use_val_data: false
  use_test_data: true
  num_samples: 1
  planning_horizon: 5
  planning_samples: 1

# WandB parameters
wandb:
  project: "pldm-overcooked"
  name: base
  entity: harshsutariya1179
  use_wandb: true
  watch_model: true
  save_model: true
  
probing:
  # Probe configuration
  probe_dynamics: true    # Probe dynamics model
  probe_reward: false     # Don't probe reward model
  probe_encoder: true     # Probe state encoder representations
  probe_predictor: true  # Don't probe dynamics predictor representations
  
  # State channel probing configuration
  state_channels:
    arch: "mlp"
    hidden_dims: [128, 64]
    
  # Agent position probing configuration
  agent_pos:
    arch: "mlp"
    hidden_dims: [128, 64]
    
  # Reward probing configuration  
  reward:
    arch: "mlp"
    hidden_dims: [64, 32]
  
  # Training parameters
  batch_size: 64
  epochs: 100             # Number of epochs for training probers
  lr: 0.0001             # Learning rate
  
  # Visualization settings
  visualize: false
  max_vis_samples: 10    # Maximum number of samples to visualize
  
  # Save options
  save_weights: false     # Whether to save prober weights to disk
  save_visualizations: false  # Whether to save visualization images